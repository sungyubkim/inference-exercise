2024-07-21-23:09:57 INFO Running script at runw/inference-phi3.py with args: ['--model', 'microsoft/Phi-3-mini-4k-instruct', '--prompt', 'Can you explain Jensen-Shannon Divergence?']
2024-07-21-23:10:17 ERROR 
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.24s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.37s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.50s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
You are not running the flash-attention implementation, expect numerical differences.

2024-07-21-23:10:17 INFO <|user|> Can you explain Jensen-Shannon Divergence?<|end|><|assistant|> Jensen-Shannon Divergence (JSD) is a method of measuring the similarity between two probability distributions. It is a symmetrical and smoothed version of the Kullback-Leibler Divergence (KLD), which is not symmetric and can be infinite if the distributions do not overlap.


The JSD is defined as the average KLD from each distribution to the midpoint distribution between them. Mathematically, for two probability distributions P and Q, the JSD is given by:


\[ JSD(P || Q) = \frac{1}{2} D_{KL}(P || M) + \frac{1}{2} D_{KL}(Q || M) \]


where \( M = \frac{1}{2}(P + Q) \) and \( D_{KL}(P || Q) = \sum P(x) \log \frac{P(x)}{Q(x)} \) is the Kullback-Leibler Divergence from P to Q.


The JSD has a range of [0, 1], where 0 indicates that the two distributions are identical, and 1 indicates that the distributions are completely different. It is widely used in various fields such as information theory, machine learning, and bioinformatics for tasks like clustering, classification, and feature selection.<|end|>

2024-07-21-23:10:17 INFO Process Process-1 finished with exit code 0
